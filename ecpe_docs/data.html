<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ecpe_data.data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ecpe_data.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np

def load_data(input_path):
    &#34;&#34;&#34;Read ECPE format training data in list of dictionary format
    
    Arguments:
        input_path (string):
            path of .txt file containing data to load

    Returns:
        docs (list[dict]):
            list of dictionaries where each element of the list corresponds to one document
    &#34;&#34;&#34;

    # List to store documents
    docs = []

    input_file = open(input_path, &#39;r&#39;, encoding=&#39;UTF-8&#39;)

    # Read line by line
    while True:
        # Read first line of documnet
        line = input_file.readline()

        # Empty line denotes end of file
        if line == &#39;&#39;: break

        # Initialize data containers
        doc = {}
        sents = []

        # split line by whitespace to get document id and length
        doc_id, doc_len = line.strip().split()
        doc_id, doc_len = int(doc_id), int(doc_len)

        # Read line containing pairs and evaluate as list of tuples
        pairs = eval(&#39;[&#39; + input_file.readline().strip() + &#39;]&#39;)
        
        # Iterate over sentences
        for i in range(doc_len):
            # Read line
            line = input_file.readline().strip()
            splitted = line.split(&#39;,&#39;)

            # First 3 elements correspond to sentence id, emotion tag, and the emotion&#39;s token span
            sent_id, emotion, phrase = splitted[0], splitted[1], splitted[2]

            # Join all remaining parts to get the sentence
            words = &#39;,&#39;.join(splitted[3:])

            # Split words by whitespace (words are pre-tokenized)
            tokens = [word.lower() for word in words.split()]

            # Append sentence data to document
            sent = {}
            sent[&#39;sent_id&#39;] = sent_id
            sent[&#39;tokens&#39;] = tokens
            sent[&#39;emotion&#39;] = emotion
            sent[&#39;phrase&#39;] = phrase
            sents.append(sent)

        # Append document data to output
        doc[&#39;doc_id&#39;] = doc_id
        doc[&#39;pairs&#39;] = pairs
        doc[&#39;sentences&#39;] = sents
        docs.append(doc)
    
    print(&#39;load data done!\n&#39;)
    return docs

def build_vocabulary(train_file_path, pad_token=&#39;PAD&#39;, oov_token=&#39;OOV&#39;):
    &#34;&#34;&#34; Build a vocabulary from input text file containing training data
    
    Arguments:
        train_file_path (str): path to txt file where each line is comma delimited and last element of each line is 
                               a tokenized sentence where tokens are separated by whitespace
        pad_token (str): Padding token
        oov_token (str): Token for out of vocabulary elements
                               
    Returns:
        words (list): list of unique tokens in dataset
        word_idx (dict): dictionary mapping unqiue tokens to id
        word_idx_rev (dict): dictionary matpping ids to unique tokens
        
    &#34;&#34;&#34;
    
    # Build vocabulary
    words = []
    inputFile1 = open(train_file_path, &#39;r&#39;, encoding=&#39;UTF-8&#39;)
    for line in inputFile1.readlines():
        line = line.strip().split(&#39;,&#39;)
        clause = line[-1]
        tokens = [word.lower() for word in clause.split()]
        words.extend(tokens)
    words = set(words) 
    word_idx = dict((c, k + 1) for k, c in enumerate(words))
    word_idx_rev = dict((k + 1, c) for k, c in enumerate(words))

    word_idx[pad_token] = 0
    word_idx_rev[0] = pad_token

    word_idx[oov_token] = len(words) + 1
    word_idx_rev[len(words) + 1] = oov_token

    words = [pad_token] + list(words) + [oov_token]
    
    return words, word_idx, word_idx_rev

&#34;&#34;&#34;TODO: Set mean and variance of distribution when sampling for words with no pre-trained vector&#34;&#34;&#34;
def load_w2v(embedding_path, vocabulary):
    &#34;&#34;&#34;Load text file holding word2vec embeddings and extract vector for each word in provided vocabulary list
       if vocab word does not exist in word2vec, then generate it randomly
   
   Arguments:
   embeddings_path (str): path to txt file containing pre-trained word embeddings
                           first line contains 2 values, number of words and embedding dimension, separated by whitespace
                           second line onwards contains pairs of unique words and embedding vectors separated by whitespace,
                           where the first element is the word and second element onwards are embedding values
    vocabulary (list): list of unique tokens 
    
    Returns:
    embedding (list): list of embedding vectors which corresponds to input vocabulary
    
    &#34;&#34;&#34;
    
    print(&#39;\nload embedding...&#39;)
    
    w2v = {}
    input_file = open(embedding_path, &#39;r&#39;, encoding=&#39;UTF-8&#39;)
    _, embedding_dim = input_file.readline().split()
    embedding_dim = int(embedding_dim)
    for line in input_file.readlines():
        line = line.strip().split(&#39; &#39;)
        word, embedding = line[0], line[1:]
        w2v[word] = embedding

    embedding = [list(np.zeros(embedding_dim))]
    hit = 0
    for item in vocabulary:
        if item in w2v:
            vec = list(map(float, w2v[item]))
            hit += 1
        else:
            # Randomly initialize
            vec = list(np.random.rand(embedding_dim) / 5. - 0.1)
        embedding.append(vec)
    
    ### add a noisy embedding in the end for out of vocabulary words
    embedding.extend([list(np.random.rand(embedding_dim) / 5. - 0.1)])

    embedding = np.array(embedding)
    
    # Prints
    print(&#39;w2v_file: {}\nall_words: {} hit_words: {}&#39;.format(embedding_path, len(vocabulary), hit))
    print(&#34;embedding.shape: {}&#34;.format(embedding.shape))
    print(&#34;load embedding done!\n&#34;)
    
    return embedding

def gen_pos_embedding(n_pos=200, dim=50):
    &#34;&#34;&#34;Generate positional embeddigns given number of positions and desired embedding dimension
       embedding for position 1 is always assigned a zero vector
    
    Arguments:
        n_pos (int): number of positions 
        dim (int): embedding dimenson
    
    Returns:
        embedding_pos (np.Array): numpy array of shape (n_pos+1, dim)
    
    &#34;&#34;&#34;
    
    embedding_pos = np.random.normal(loc=0.0, scale=0.1, size=(n_pos+1, dim))
    embedding_pos[0] = 0.0
    print(&#34;embedding_pos.shape: {}&#34;.format(embedding_pos.shape))
    return embedding_pos</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ecpe_data.data.build_vocabulary"><code class="name flex">
<span>def <span class="ident">build_vocabulary</span></span>(<span>train_file_path, pad_token='PAD', oov_token='OOV')</span>
</code></dt>
<dd>
<div class="desc"><p>Build a vocabulary from input text file containing training data</p>
<h2 id="arguments">Arguments</h2>
<p>train_file_path (str): path to txt file where each line is comma delimited and last element of each line is
a tokenized sentence where tokens are separated by whitespace
pad_token (str): Padding token
oov_token (str): Token for out of vocabulary elements</p>
<h2 id="returns">Returns</h2>
<p>words (list): list of unique tokens in dataset
word_idx (dict): dictionary mapping unqiue tokens to id
word_idx_rev (dict): dictionary matpping ids to unique tokens</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_vocabulary(train_file_path, pad_token=&#39;PAD&#39;, oov_token=&#39;OOV&#39;):
    &#34;&#34;&#34; Build a vocabulary from input text file containing training data
    
    Arguments:
        train_file_path (str): path to txt file where each line is comma delimited and last element of each line is 
                               a tokenized sentence where tokens are separated by whitespace
        pad_token (str): Padding token
        oov_token (str): Token for out of vocabulary elements
                               
    Returns:
        words (list): list of unique tokens in dataset
        word_idx (dict): dictionary mapping unqiue tokens to id
        word_idx_rev (dict): dictionary matpping ids to unique tokens
        
    &#34;&#34;&#34;
    
    # Build vocabulary
    words = []
    inputFile1 = open(train_file_path, &#39;r&#39;, encoding=&#39;UTF-8&#39;)
    for line in inputFile1.readlines():
        line = line.strip().split(&#39;,&#39;)
        clause = line[-1]
        tokens = [word.lower() for word in clause.split()]
        words.extend(tokens)
    words = set(words) 
    word_idx = dict((c, k + 1) for k, c in enumerate(words))
    word_idx_rev = dict((k + 1, c) for k, c in enumerate(words))

    word_idx[pad_token] = 0
    word_idx_rev[0] = pad_token

    word_idx[oov_token] = len(words) + 1
    word_idx_rev[len(words) + 1] = oov_token

    words = [pad_token] + list(words) + [oov_token]
    
    return words, word_idx, word_idx_rev</code></pre>
</details>
</dd>
<dt id="ecpe_data.data.gen_pos_embedding"><code class="name flex">
<span>def <span class="ident">gen_pos_embedding</span></span>(<span>n_pos=200, dim=50)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate positional embeddigns given number of positions and desired embedding dimension
embedding for position 1 is always assigned a zero vector</p>
<h2 id="arguments">Arguments</h2>
<p>n_pos (int): number of positions
dim (int): embedding dimenson</p>
<h2 id="returns">Returns</h2>
<p>embedding_pos (np.Array): numpy array of shape (n_pos+1, dim)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gen_pos_embedding(n_pos=200, dim=50):
    &#34;&#34;&#34;Generate positional embeddigns given number of positions and desired embedding dimension
       embedding for position 1 is always assigned a zero vector
    
    Arguments:
        n_pos (int): number of positions 
        dim (int): embedding dimenson
    
    Returns:
        embedding_pos (np.Array): numpy array of shape (n_pos+1, dim)
    
    &#34;&#34;&#34;
    
    embedding_pos = np.random.normal(loc=0.0, scale=0.1, size=(n_pos+1, dim))
    embedding_pos[0] = 0.0
    print(&#34;embedding_pos.shape: {}&#34;.format(embedding_pos.shape))
    return embedding_pos</code></pre>
</details>
</dd>
<dt id="ecpe_data.data.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>input_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Read ECPE format training data in list of dictionary format</p>
<h2 id="arguments">Arguments</h2>
<p>input_path (string):
path of .txt file containing data to load</p>
<h2 id="returns">Returns</h2>
<p>docs (list[dict]):
list of dictionaries where each element of the list corresponds to one document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(input_path):
    &#34;&#34;&#34;Read ECPE format training data in list of dictionary format
    
    Arguments:
        input_path (string):
            path of .txt file containing data to load

    Returns:
        docs (list[dict]):
            list of dictionaries where each element of the list corresponds to one document
    &#34;&#34;&#34;

    # List to store documents
    docs = []

    input_file = open(input_path, &#39;r&#39;, encoding=&#39;UTF-8&#39;)

    # Read line by line
    while True:
        # Read first line of documnet
        line = input_file.readline()

        # Empty line denotes end of file
        if line == &#39;&#39;: break

        # Initialize data containers
        doc = {}
        sents = []

        # split line by whitespace to get document id and length
        doc_id, doc_len = line.strip().split()
        doc_id, doc_len = int(doc_id), int(doc_len)

        # Read line containing pairs and evaluate as list of tuples
        pairs = eval(&#39;[&#39; + input_file.readline().strip() + &#39;]&#39;)
        
        # Iterate over sentences
        for i in range(doc_len):
            # Read line
            line = input_file.readline().strip()
            splitted = line.split(&#39;,&#39;)

            # First 3 elements correspond to sentence id, emotion tag, and the emotion&#39;s token span
            sent_id, emotion, phrase = splitted[0], splitted[1], splitted[2]

            # Join all remaining parts to get the sentence
            words = &#39;,&#39;.join(splitted[3:])

            # Split words by whitespace (words are pre-tokenized)
            tokens = [word.lower() for word in words.split()]

            # Append sentence data to document
            sent = {}
            sent[&#39;sent_id&#39;] = sent_id
            sent[&#39;tokens&#39;] = tokens
            sent[&#39;emotion&#39;] = emotion
            sent[&#39;phrase&#39;] = phrase
            sents.append(sent)

        # Append document data to output
        doc[&#39;doc_id&#39;] = doc_id
        doc[&#39;pairs&#39;] = pairs
        doc[&#39;sentences&#39;] = sents
        docs.append(doc)
    
    print(&#39;load data done!\n&#39;)
    return docs</code></pre>
</details>
</dd>
<dt id="ecpe_data.data.load_w2v"><code class="name flex">
<span>def <span class="ident">load_w2v</span></span>(<span>embedding_path, vocabulary)</span>
</code></dt>
<dd>
<div class="desc"><p>Load text file holding word2vec embeddings and extract vector for each word in provided vocabulary list
if vocab word does not exist in word2vec, then generate it randomly</p>
<p>Arguments:
embeddings_path (str): path to txt file containing pre-trained word embeddings
first line contains 2 values, number of words and embedding dimension, separated by whitespace
second line onwards contains pairs of unique words and embedding vectors separated by whitespace,
where the first element is the word and second element onwards are embedding values
vocabulary (list): list of unique tokens </p>
<p>Returns:
embedding (list): list of embedding vectors which corresponds to input vocabulary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_w2v(embedding_path, vocabulary):
    &#34;&#34;&#34;Load text file holding word2vec embeddings and extract vector for each word in provided vocabulary list
       if vocab word does not exist in word2vec, then generate it randomly
   
   Arguments:
   embeddings_path (str): path to txt file containing pre-trained word embeddings
                           first line contains 2 values, number of words and embedding dimension, separated by whitespace
                           second line onwards contains pairs of unique words and embedding vectors separated by whitespace,
                           where the first element is the word and second element onwards are embedding values
    vocabulary (list): list of unique tokens 
    
    Returns:
    embedding (list): list of embedding vectors which corresponds to input vocabulary
    
    &#34;&#34;&#34;
    
    print(&#39;\nload embedding...&#39;)
    
    w2v = {}
    input_file = open(embedding_path, &#39;r&#39;, encoding=&#39;UTF-8&#39;)
    _, embedding_dim = input_file.readline().split()
    embedding_dim = int(embedding_dim)
    for line in input_file.readlines():
        line = line.strip().split(&#39; &#39;)
        word, embedding = line[0], line[1:]
        w2v[word] = embedding

    embedding = [list(np.zeros(embedding_dim))]
    hit = 0
    for item in vocabulary:
        if item in w2v:
            vec = list(map(float, w2v[item]))
            hit += 1
        else:
            # Randomly initialize
            vec = list(np.random.rand(embedding_dim) / 5. - 0.1)
        embedding.append(vec)
    
    ### add a noisy embedding in the end for out of vocabulary words
    embedding.extend([list(np.random.rand(embedding_dim) / 5. - 0.1)])

    embedding = np.array(embedding)
    
    # Prints
    print(&#39;w2v_file: {}\nall_words: {} hit_words: {}&#39;.format(embedding_path, len(vocabulary), hit))
    print(&#34;embedding.shape: {}&#34;.format(embedding.shape))
    print(&#34;load embedding done!\n&#34;)
    
    return embedding</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ecpe_data" href="index.html">ecpe_data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ecpe_data.data.build_vocabulary" href="#ecpe_data.data.build_vocabulary">build_vocabulary</a></code></li>
<li><code><a title="ecpe_data.data.gen_pos_embedding" href="#ecpe_data.data.gen_pos_embedding">gen_pos_embedding</a></code></li>
<li><code><a title="ecpe_data.data.load_data" href="#ecpe_data.data.load_data">load_data</a></code></li>
<li><code><a title="ecpe_data.data.load_w2v" href="#ecpe_data.data.load_w2v">load_w2v</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>